<?xml version="1.0"?><!DOCTYPE resource-agent SYSTEM "ra-api-1.dtd">
<resource-agent name="pacemaker-schedulerd">
  <version>1.1</version>
  <longdesc lang="en">Cluster options used by Pacemaker&apos;s scheduler</longdesc>
  <shortdesc lang="en">Pacemaker scheduler options</shortdesc>
  <parameters>
    <parameter name="no-quorum-policy">
      <longdesc lang="en">What to do when the cluster does not have quorum  Allowed values: stop, freeze, ignore, demote, suicide</longdesc>
      <shortdesc lang="en">What to do when the cluster does not have quorum</shortdesc>
      <content type="select" default="stop">
        <option value="stop" />
        <option value="freeze" />
        <option value="ignore" />
        <option value="demote" />
        <option value="suicide" />
      </content>
    </parameter>
    <parameter name="symmetric-cluster">
      <longdesc lang="en">Whether resources can run on any node by default</longdesc>
      <shortdesc lang="en">Whether resources can run on any node by default</shortdesc>
      <content type="boolean" default="true"/>
    </parameter>
    <parameter name="maintenance-mode">
      <longdesc lang="en">Whether the cluster should refrain from monitoring, starting, and stopping resources</longdesc>
      <shortdesc lang="en">Whether the cluster should refrain from monitoring, starting, and stopping resources</shortdesc>
      <content type="boolean" default="false"/>
    </parameter>
    <parameter name="start-failure-is-fatal">
      <longdesc lang="en">When true, the cluster will immediately ban a resource from a node if it fails to start there. When false, the cluster will instead check the resource&apos;s fail count against its migration-threshold.</longdesc>
      <shortdesc lang="en">Whether a start failure should prevent a resource from being recovered on the same node</shortdesc>
      <content type="boolean" default="true"/>
    </parameter>
    <parameter name="enable-startup-probes">
      <longdesc lang="en">Whether the cluster should check for active resources during start-up</longdesc>
      <shortdesc lang="en">Whether the cluster should check for active resources during start-up</shortdesc>
      <content type="boolean" default="true"/>
    </parameter>
    <parameter name="shutdown-lock">
      <longdesc lang="en">When true, resources active on a node when it is cleanly shut down are kept &quot;locked&quot; to that node (not allowed to run elsewhere) until they start again on that node after it rejoins (or for at most shutdown-lock-limit, if set). Stonith resources and Pacemaker Remote connections are never locked. Clone and bundle instances and the promoted role of promotable clones are currently never locked, though support could be added in a future release.</longdesc>
      <shortdesc lang="en">Whether to lock resources to a cleanly shut down node</shortdesc>
      <content type="boolean" default="false"/>
    </parameter>
    <parameter name="shutdown-lock-limit">
      <longdesc lang="en">If shutdown-lock is true and this is set to a nonzero time duration, shutdown locks will expire after this much time has passed since the shutdown was initiated, even if the node has not rejoined.</longdesc>
      <shortdesc lang="en">Do not lock resources to a cleanly shut down node longer than this</shortdesc>
      <content type="time" default="0"/>
    </parameter>
    <parameter name="stonith-enabled">
      <longdesc lang="en">If false, unresponsive nodes are immediately assumed to be harmless, and resources that were active on them may be recovered elsewhere. This can result in a &quot;split-brain&quot; situation, potentially leading to data loss and/or service unavailability.</longdesc>
      <shortdesc lang="en">*** Advanced Use Only *** Whether nodes may be fenced as part of recovery</shortdesc>
      <content type="boolean" default="true"/>
    </parameter>
    <parameter name="stonith-action">
      <longdesc lang="en">Action to send to fence device when a node needs to be fenced (&quot;poweroff&quot; is a deprecated alias for &quot;off&quot;)  Allowed values: reboot, off, poweroff</longdesc>
      <shortdesc lang="en">Action to send to fence device when a node needs to be fenced (&quot;poweroff&quot; is a deprecated alias for &quot;off&quot;)</shortdesc>
      <content type="select" default="reboot">
        <option value="reboot" />
        <option value="off" />
        <option value="poweroff" />
      </content>
    </parameter>
    <parameter name="stonith-timeout">
      <longdesc lang="en">This value is not used by Pacemaker, but is kept for backward compatibility, and certain legacy fence agents might use it.</longdesc>
      <shortdesc lang="en">*** Advanced Use Only *** Unused by Pacemaker</shortdesc>
      <content type="time" default="60s"/>
    </parameter>
    <parameter name="have-watchdog">
      <longdesc lang="en">This is set automatically by the cluster according to whether SBD is detected to be in use. User-configured values are ignored. The value `true` is meaningful if diskless SBD is used and `stonith-watchdog-timeout` is nonzero. In that case, if fencing is required, watchdog-based self-fencing will be performed via SBD without requiring a fencing resource explicitly configured.</longdesc>
      <shortdesc lang="en">Whether watchdog integration is enabled</shortdesc>
      <content type="boolean" default="false"/>
    </parameter>
    <parameter name="concurrent-fencing">
      <longdesc lang="en">Allow performing fencing operations in parallel</longdesc>
      <shortdesc lang="en">Allow performing fencing operations in parallel</shortdesc>
      <content type="boolean" default="true"/>
    </parameter>
    <parameter name="startup-fencing">
      <longdesc lang="en">Setting this to false may lead to a &quot;split-brain&quot; situation,potentially leading to data loss and/or service unavailability.</longdesc>
      <shortdesc lang="en">*** Advanced Use Only *** Whether to fence unseen nodes at start-up</shortdesc>
      <content type="boolean" default="true"/>
    </parameter>
    <parameter name="priority-fencing-delay">
      <longdesc lang="en">Apply specified delay for the fencings that are targeting the lost nodes with the highest total resource priority in case we don&apos;t have the majority of the nodes in our cluster partition, so that the more significant nodes potentially win any fencing match, which is especially meaningful under split-brain of 2-node cluster. A promoted resource instance takes the base priority + 1 on calculation if the base priority is not 0. Any static/random delays that are introduced by `pcmk_delay_base/max` configured for the corresponding fencing resources will be added to this delay. This delay should be significantly greater than, safely twice, the maximum `pcmk_delay_base/max`. By default, priority fencing delay is disabled.</longdesc>
      <shortdesc lang="en">Apply fencing delay targeting the lost nodes with the highest total resource priority</shortdesc>
      <content type="time" default="0"/>
    </parameter>
    <parameter name="node-pending-timeout">
      <longdesc lang="en">
        Fence nodes that do not join the controller process group within this much time after joining the cluster, to allow the cluster to continue managing resources. A value of 0 means never fence pending nodes. Setting the value to 2h means fence nodes after 2 hours.
      </longdesc>
      <shortdesc lang="en">
        How long to wait for a node that has joined the cluster to join the controller process group
      </shortdesc>
      <content type="time" default="0"/>
    </parameter>
    <parameter name="cluster-delay">
      <longdesc lang="en">The node elected Designated Controller (DC) will consider an action failed if it does not get a response from the node executing the action within this time (after considering the action&apos;s own timeout). The &quot;correct&quot; value will depend on the speed and load of your network and cluster nodes.</longdesc>
      <shortdesc lang="en">Maximum time for node-to-node communication</shortdesc>
      <content type="time" default="60s"/>
    </parameter>
    <parameter name="batch-limit">
      <longdesc lang="en">The &quot;correct&quot; value will depend on the speed and load of your network and cluster nodes. If set to 0, the cluster will impose a dynamically calculated limit when any node has a high load.</longdesc>
      <shortdesc lang="en">Maximum number of jobs that the cluster may execute in parallel across all nodes</shortdesc>
      <content type="integer" default="0"/>
    </parameter>
    <parameter name="migration-limit">
      <longdesc lang="en">The number of live migration actions that the cluster is allowed to execute in parallel on a node (-1 means no limit)</longdesc>
      <shortdesc lang="en">The number of live migration actions that the cluster is allowed to execute in parallel on a node (-1 means no limit)</shortdesc>
      <content type="integer" default="-1"/>
    </parameter>
    <parameter name="stop-all-resources">
      <longdesc lang="en">Whether the cluster should stop all active resources</longdesc>
      <shortdesc lang="en">Whether the cluster should stop all active resources</shortdesc>
      <content type="boolean" default="false"/>
    </parameter>
    <parameter name="stop-orphan-resources">
      <longdesc lang="en">Whether to stop resources that were removed from the configuration</longdesc>
      <shortdesc lang="en">Whether to stop resources that were removed from the configuration</shortdesc>
      <content type="boolean" default="true"/>
    </parameter>
    <parameter name="stop-orphan-actions">
      <longdesc lang="en">Whether to cancel recurring actions removed from the configuration</longdesc>
      <shortdesc lang="en">Whether to cancel recurring actions removed from the configuration</shortdesc>
      <content type="boolean" default="true"/>
    </parameter>
    <parameter name="remove-after-stop">
      <longdesc lang="en">Values other than default are poorly tested and potentially dangerous. This option will be removed in a future release.</longdesc>
      <shortdesc lang="en">*** Deprecated *** Whether to remove stopped resources from the executor</shortdesc>
      <content type="boolean" default="false"/>
    </parameter>
    <parameter name="pe-error-series-max">
      <longdesc lang="en">Zero to disable, -1 to store unlimited.</longdesc>
      <shortdesc lang="en">The number of scheduler inputs resulting in errors to save</shortdesc>
      <content type="integer" default="-1"/>
    </parameter>
    <parameter name="pe-warn-series-max">
      <longdesc lang="en">Zero to disable, -1 to store unlimited.</longdesc>
      <shortdesc lang="en">The number of scheduler inputs resulting in warnings to save</shortdesc>
      <content type="integer" default="5000"/>
    </parameter>
    <parameter name="pe-input-series-max">
      <longdesc lang="en">Zero to disable, -1 to store unlimited.</longdesc>
      <shortdesc lang="en">The number of scheduler inputs without errors or warnings to save</shortdesc>
      <content type="integer" default="4000"/>
    </parameter>
    <parameter name="node-health-strategy">
      <longdesc lang="en">Requires external entities to create node attributes (named with the prefix &quot;#health&quot;) with values &quot;red&quot;, &quot;yellow&quot;, or &quot;green&quot;.  Allowed values: none, migrate-on-red, only-green, progressive, custom</longdesc>
      <shortdesc lang="en">How cluster should react to node health attributes</shortdesc>
      <content type="select" default="none">
        <option value="none" />
        <option value="migrate-on-red" />
        <option value="only-green" />
        <option value="progressive" />
        <option value="custom" />
      </content>
    </parameter>
    <parameter name="node-health-base">
      <longdesc lang="en">Only used when node-health-strategy is set to progressive.</longdesc>
      <shortdesc lang="en">Base health score assigned to a node</shortdesc>
      <content type="integer" default="0"/>
    </parameter>
    <parameter name="node-health-green">
      <longdesc lang="en">Only used when node-health-strategy is set to custom or progressive.</longdesc>
      <shortdesc lang="en">The score to use for a node health attribute whose value is &quot;green&quot;</shortdesc>
      <content type="integer" default="0"/>
    </parameter>
    <parameter name="node-health-yellow">
      <longdesc lang="en">Only used when node-health-strategy is set to custom or progressive.</longdesc>
      <shortdesc lang="en">The score to use for a node health attribute whose value is &quot;yellow&quot;</shortdesc>
      <content type="integer" default="0"/>
    </parameter>
    <parameter name="node-health-red">
      <longdesc lang="en">Only used when node-health-strategy is set to custom or progressive.</longdesc>
      <shortdesc lang="en">The score to use for a node health attribute whose value is &quot;red&quot;</shortdesc>
      <content type="integer" default="-INFINITY"/>
    </parameter>
    <parameter name="placement-strategy">
      <longdesc lang="en">How the cluster should allocate resources to nodes  Allowed values: default, utilization, minimal, balanced</longdesc>
      <shortdesc lang="en">How the cluster should allocate resources to nodes</shortdesc>
      <content type="select" default="default">
        <option value="default" />
        <option value="utilization" />
        <option value="minimal" />
        <option value="balanced" />
      </content>
    </parameter>
  </parameters>
</resource-agent>
